#  How to find m and b?

![lr](https://github.com/user-attachments/assets/67a54634-25c7-447c-a861-d7739ea3aea8)

| Symbol | Meaning                            | Example             |
| ------ | ---------------------------------- | ------------------- |
| `y`    | Output / Dependent variable        | Package ðŸ’°          |
| `x`    | Input / Independent variable       | CGPA ðŸŽ“             |
| `m`    | Slope of the line                  | 1.5 (growth factor) |
| `b`    | Intercept (where line cuts y-axis) | 2.0 (base salary)   |


we can solve above equetion by two method

1. Closed form solution

direct formula

also called Ordinary least square

2. Non - closed form solution

approximation based technique

where we use differentiation

called Gradient Descent

Why there is Gradient Descent if we have direct formula?

because in higher dimmension, (more feature or columns) to perform calculation become complex and takes more energy

but in scikit class there are OLS is implemented

we have SGDRegressor() that uses GD

#

# Invent the formula from Scratch

![lr1](https://github.com/user-attachments/assets/4a8db174-7c50-4ed1-8651-9778a1fdd697)

![lr2](https://github.com/user-attachments/assets/fa216526-d92e-4acd-85f4-22a015c6cd0f)

Suppose we have cgpa vs package data

![lr3](https://github.com/user-attachments/assets/93c27000-68d2-46a0-9d86-57db4a98a658)

if we have perfect linear data then we drow a line that go throgh each of the data points.

but in our case or in real word our data always or 99.99% sort of linear.

we cant drow perfect linear line but we can drow best fit line, that tries to go cover each data point very closely.

and this will do some error for each like

har data point ke liye hamara line thoda galati karega, kabhi data ke uper jayega kabhi niche

that will be the error for each data point

also it is the distance between actual and predicted value

and can be denoted as d1, d2, d3, .... dn (n is last data point or no. of data points)

![l3](https://github.com/user-attachments/assets/8673c1fe-c882-4694-b562-78705adc0957)

so we want total error then we add it all

E = d1 + d2 + d3 + ... + dn

![l5](https://github.com/user-attachments/assets/86974079-6677-4645-891d-412fed82f0c9)

![l4](https://github.com/user-attachments/assets/3d49402f-2764-4937-a1bd-763b806bff9f)

It is error function (loss function)

we take square becasue we have to ensures no cancellation of positive/negative errors

why we not take mod of data points 

because 

1. We have to punish outlier
2. We have to differntiate the equetion afterwords, and graph of mod is not continuous at origin

Ok. so before we want to find out the line that passes closely by each data points, 
now our goal is to find the minimum value of above error function, 
we want value m and b such that e is minimum
jo line sabse kam galati karega vahi hamara best fit line hoga.

if we observe the d , it is nothing but it is the difference between predicted value and actual value on y axis 

![l6](https://github.com/user-attachments/assets/aeafeacc-dd50-4e97-ae3d-185dd3392327)

we want Total measure of mistakes

![l7](https://github.com/user-attachments/assets/3c59b622-a2f4-4e30-8842-5aec34895a1e)

we required value of m and b such that E is minimum

m and b is like tuning knob, to change the values of E

becasue others (x and y) are fix (given in data)


suppose our b = 0

means we only change the slop but we can not change y - intercept

![l8](https://github.com/user-attachments/assets/b1298b5a-4234-44f7-a5e3-16dbe847d18a)

for this our E vs m graph 

![l9](https://github.com/user-attachments/assets/232b4071-f676-47b9-96fd-3d101a3c6059)

first it is decreases then it increases

same for if m = 0

we only change y - intercept

aap sirf uper ya niche ja sakte ho, angle fix rahega

so now we want minimum value of E in terms of (m, b) for that we take derivative of F(x,y) = 0

We use calculus â€” specifically, partial derivatives â€” to find the minimum point of the function.

What is Partial Derivatives 

https://github.com/kiransalve/Machine-Learning-Tutorial/blob/main/Linear%20Regression/Simple%20Linear%20Regression/3.%20Partial%20Derivative.md

# ðŸ“Š Deriving Intercept \( b \) in Simple Linear Regression

We start with the **total squared error** formula in linear regression:

\[
E(m, b) = \sum_{i=1}^{n} (y_i - mx_i - b)^2
\]

---

## ðŸŽ¯ Goal: Find value of \( b \) that minimizes the error

We'll differentiate \( E(m, b) \) **with respect to \( b \)** and set the derivative to **zero**.

---

## âœ… Step 1: Take Partial Derivative of Error w.r.t. \( b \)

\[
\frac{\partial E}{\partial b} = \sum_{i=1}^{n} 2(y_i - mx_i - b)(-1)
= -2 \sum_{i=1}^{n} (y_i - mx_i - b)
\]

---

## âœ… Step 2: Set Derivative to Zero

\[
-2 \sum_{i=1}^{n} (y_i - mx_i - b) = 0
\]

Divide both sides by -2:

\[
\sum_{i=1}^{n} (y_i - mx_i - b) = 0
\]

---

## âœ… Step 3: Expand the Summation

\[
\sum y_i - m \sum x_i - \sum b = 0
\]

Since \( b \) is a constant:

\[
\sum b = nb
\]

So the equation becomes:

\[
\sum y_i - m \sum x_i - nb = 0
\]

---

## âœ… Step 4: Solve for \( b \)

\[
nb = \sum y_i - m \sum x_i
\]

\[
\boxed{b = \frac{\sum y_i - m \sum x_i}{n}}
\]

---

## âœ… Step 5: Express Using Means

Let:

- \( \bar{x} = \frac{1}{n} \sum x_i \)
- \( \bar{y} = \frac{1}{n} \sum y_i \)

Then:

\[
\boxed{b = \bar{y} - m \bar{x}}
\]

---

## ðŸ“Œ Interpretation:

This shows that the **intercept** is what's left in the predicted value after accounting for the contribution of the slope \( m \cdot x \).  
Even if \( x = 0 \), the model still predicts \( b \), which represents the **base prediction**.

---
