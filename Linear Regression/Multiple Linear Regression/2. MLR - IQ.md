What is Multiple Linear Regression?

Multiple Linear Regression is a method used to predict a value based on two or more inputs features.

Difference between SLR and MLR?

SLR have one indenpendant feature and MLR have two or more independant features

SLR - y = Bo + B1X1

MRL - y = Bo + B1X1 + B2X2 + B3X3 .. + BnXn

SLR - one streight line that try to go throgh all data points

MLR - plane or hyperplane that try to cut all data points

SLR - Linearity, independence, homoscedasticity, normality of residuals

MLR - Same assumptions, but also includes no multicollinearity


What assumptions does multiple linear regression make?

Linearity

The relationship between the dependent variable (Y) and the independent variables (X₁, X₂, ..., Xₙ) is linear.

Check with: Residual vs. Fitted plot — it should not show curves or patterns.

Independence of Errors

The residuals (errors) should be independent of each other.

Homoscedasticity (Constant Variance of Errors)

The variance of residuals should be constant across all levels of the independent variables.

No Multicollinearity

The independent variables should not be highly correlated with each other.

Normality of Residuals

The residuals should be normally distributed.


What is Multicollinearity and How Do You Detect It?

Multicollinearity occurs when two or more independent variables in a regression model are highly correlated with each other.

To Detect Multicollinearity Correlation Matrix - corr_matrix = df.corr()

Check pairwise correlations between features

If correlation > 0.8 or < -0.8, multicollinearity may be present

To Handle Multicollinearity

Remove or combine correlated features

Use PCA (Principal Component Analysis)

Use Ridge regression (adds penalty to reduce coefficient instability)

What does each coefficient represent in Multiple Linear Regression (MLR)?

β₀ (Intercept)

The offset value of Y when all X variables are 0.

It is the baseline prediction.

βᵢ (Coefficients for predictors Xᵢ)

Represents the average change in Y for a 1-unit increase in Xᵢ, keeping all other predictors constant.

What is the role of the intercept (β₀) in Multiple Linear Regression?

The intercept (β₀) is the predicted value of the dependent variable (Y) when all independent variables (X₁, X₂, ..., Xₙ) are equal to zero.
Imagine you're predicting salary using experience and education level.

If both experience and education = 0, the intercept is the base salary for someone with no education and no experience.

Sometimes, when X=0 has no practical meaning, the intercept may not be interpretable.

Example: predicting house price where square footage = 0 (a useless real-world scenario)

Can we remove the intercept?

Yes, by setting fit_intercept=False in libraries like scikit-learn.

But this is done only when you're sure that the line must go through the origin (rare in real-world cases).

What is R-squared and what are its limitations?

R-squared is give you how much your linear regreesion line is better than mean line

It also called Coefficient of determination or goodness of fit

it is fraction of error of square of regression line and error of square of mean line

= 1 - (rror of square of regression line / error of square of mean line)

= 1 - (SSR / SSM)

If R2 score is 0 then your model didn't explain any variability of input columns

If R2 score is 1 then your model explain all the variablity of input columns

If R2 score is 0.80 then your model explain 80% variablity of input columns

If R2 score is negetive - means your mean line is better perform than your lr line.




